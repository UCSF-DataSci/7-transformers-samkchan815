{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b25df5d",
   "metadata": {},
   "source": [
    "# Part 3: Prompt Engineering Basics\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this part, you'll experiment with different prompting techniques to improve the quality of responses from Large Language Models (LLMs). You'll compare zero-shot, one-shot, and few-shot prompting approaches and document which works best for different types of questions.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand different prompting techniques\n",
    "- Compare zero-shot, one-shot, and few-shot prompting\n",
    "- Analyze the impact of prompt design on response quality\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cf754da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab0dd80",
   "metadata": {},
   "source": [
    "## 1. Understanding Prompting Techniques\n",
    "\n",
    "LLMs can be prompted in different ways to get better responses:\n",
    "\n",
    "1. **Zero-shot prompting**: Asking the model a question directly without examples\n",
    "2. **One-shot prompting**: Providing one example before asking your question\n",
    "3. **Few-shot prompting**: Providing multiple examples before asking your question\n",
    "\n",
    "## 2. Creating Prompting Templates\n",
    "\n",
    "Your first task is to create templates for different prompting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a06b82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot prompt:\n",
      "Question: What foods should be avoided by patients with gout?\n",
      "Answer:\n",
      "\n",
      "One-shot prompt:\n",
      "Question: What are the symptoms of gout?\n",
      "Answer: Gout symptoms include sudden severe pain, swelling, redness, and tenderness in joints, often the big toe.\n",
      "\n",
      "Question: What foods should be avoided by patients with gout?\n",
      "Answer:\n",
      "\n",
      "Few-shot prompt:\n",
      "Question: What are the symptoms of gout?\n",
      "Answer: Gout symptoms include sudden severe pain, swelling, redness, and tenderness in joints, often the big toe.\n",
      "\n",
      "Question: How is gout diagnosed?\n",
      "Answer: Gout is diagnosed through physical examination, medical history, blood tests for uric acid levels, and joint fluid analysis to look for urate crystals.\n",
      "\n",
      "Question: What foods should be avoided by patients with gout?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Define a question to experiment with\n",
    "question = \"What foods should be avoided by patients with gout?\"\n",
    "\n",
    "# Example for one-shot and few-shot prompting\n",
    "example_q = \"What are the symptoms of gout?\"\n",
    "example_a = \"Gout symptoms include sudden severe pain, swelling, redness, and tenderness in joints, often the big toe.\"\n",
    "\n",
    "# Examples for few-shot prompting\n",
    "examples = [\n",
    "    (\"What are the symptoms of gout?\",\n",
    "     \"Gout symptoms include sudden severe pain, swelling, redness, and tenderness in joints, often the big toe.\"),\n",
    "    (\"How is gout diagnosed?\",\n",
    "     \"Gout is diagnosed through physical examination, medical history, blood tests for uric acid levels, and joint fluid analysis to look for urate crystals.\")\n",
    "]\n",
    "\n",
    "# TODO: Create prompting templates\n",
    "# Zero-shot template (just the question)\n",
    "zero_shot_template = \"Question: {question}\\nAnswer:\"\n",
    "\n",
    "# One-shot template (one example + the question)\n",
    "one_shot_template = \"\"\"Question: {example_q}\n",
    "Answer: {example_a}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Few-shot template (multiple examples + the question)\n",
    "few_shot_template = \"\"\"Question: {examples[0][0]}\n",
    "Answer: {examples[0][1]}\n",
    "\n",
    "Question: {examples[1][0]}\n",
    "Answer: {examples[1][1]}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# TODO: Format the templates with your question and examples\n",
    "zero_shot_prompt = zero_shot_template.format(question=question)\n",
    "one_shot_prompt = one_shot_template.format(example_q=example_q, example_a=example_a, question=question)\n",
    "# For few-shot, you'll need to format it with the examples list\n",
    "few_shot_prompt = few_shot_template.format(examples=examples, question=question)\n",
    "\n",
    "print(\"Zero-shot prompt:\")\n",
    "print(zero_shot_prompt)\n",
    "print(\"\\nOne-shot prompt:\")\n",
    "print(one_shot_prompt)\n",
    "print(\"\\nFew-shot prompt:\")\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d338f577",
   "metadata": {},
   "source": [
    "## 3. Connecting to the LLM API\n",
    "\n",
    "Next, implement a function to send prompts to an LLM API and get responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3efb0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sakachan/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/sakachan/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a swollen rectum'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def get_llm_response(prompt, model_name=\"google/flan-t5-base\", api_key=None):\n",
    "    \"\"\"Get a response from the LLM based on the prompt\"\"\"\n",
    "    # TODO: Implement the get_llm_response function\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "    payload = {\"inputs\": prompt}\n",
    "    input_txt = payload.get(\"inputs\", \"\")\n",
    "\n",
    "    inputs = tokenizer(input_txt, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# TODO: Test your get_llm_response function with different prompts\n",
    "prompt = \"What are the symptoms of diabetes?\"\n",
    "get_llm_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f6472",
   "metadata": {},
   "source": [
    "## 4. Comparing Prompting Strategies\n",
    "\n",
    "Now, let's compare the different prompting strategies on a set of healthcare questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b29e9ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'What foods should be avoided by patients with gout?': {'zero-shot': 'Question: What foods should be avoided by patients with gout?\\nAnswer:', 'one-shot': 'Question: What foods should be avoided by patients with gout?\\nAnswer: Sugary drinks\\n\\nQuestion: What foods should be avoided by patients with gout?\\nAnswer:', 'few-shot': 'Question: What foods should be avoided by patients with gout?\\nAnswer: Sugary drinks\\n\\nQuestion: What foods should be avoided by patients with gout?\\nAnswer: high purine foods\\n\\nQuestion: What foods should be avoided by patients with gout?\\nAnswer:'}, 'What medications are commonly prescribed for gout?': {'zero-shot': 'Question: What medications are commonly prescribed for gout?\\nAnswer:', 'one-shot': 'Question: What medications are commonly prescribed for gout?\\nAnswer: Colchicine\\n\\nQuestion: What medications are commonly prescribed for gout?\\nAnswer:', 'few-shot': 'Question: What medications are commonly prescribed for gout?\\nAnswer: Colchicine\\n\\nQuestion: What medications are commonly prescribed for gout?\\nAnswer: Nonsteroidal anti-inflammatory drugs (NSAIDs)\\n\\nQuestion: What medications are commonly prescribed for gout?\\nAnswer:'}, 'How can gout flares be prevented?': {'zero-shot': 'Question: How can gout flares be prevented?\\nAnswer:', 'one-shot': 'Question: How can gout flares be prevented?\\nAnswer: Reduce uric acid levels\\n\\nQuestion: How can gout flares be prevented?\\nAnswer:', 'few-shot': 'Question: How can gout flares be prevented?\\nAnswer: Reduce uric acid levels\\n\\nQuestion: How can gout flares be prevented?\\nAnswer: Maintain healthy weight\\n\\nQuestion: How can gout flares be prevented?\\nAnswer:'}, 'Is gout related to diet?': {'zero-shot': 'Question: Is gout related to diet?\\nAnswer:', 'one-shot': 'Question: Is gout related to diet?\\nAnswer: Yes, uric acid build-up can cause gout\\n\\nQuestion: Is gout related to diet?\\nAnswer:', 'few-shot': 'Question: Is gout related to diet?\\nAnswer: Yes, uric acid build-up can cause gout\\n\\nQuestion: Is gout related to diet?\\nAnswer: Yes, diet choices can affect gout\\n\\nQuestion: Is gout related to diet?\\nAnswer:'}, 'Can gout be cured permanently?': {'zero-shot': 'Question: Can gout be cured permanently?\\nAnswer:', 'one-shot': 'Question: Can gout be cured permanently?\\nAnswer: No, it cannot be cured permanently.\\n\\nQuestion: Can gout be cured permanently?\\nAnswer:', 'few-shot': 'Question: Can gout be cured permanently?\\nAnswer: No, it cannot be cured permanently.\\n\\nQuestion: Can gout be cured permanently?\\nAnswer: There is no cure, but management is possible.\\n\\nQuestion: Can gout be cured permanently?\\nAnswer:'}}\n"
     ]
    }
   ],
   "source": [
    "# List of healthcare questions to test\n",
    "questions = [\n",
    "    \"What foods should be avoided by patients with gout?\",\n",
    "    \"What medications are commonly prescribed for gout?\",\n",
    "    \"How can gout flares be prevented?\",\n",
    "    \"Is gout related to diet?\",\n",
    "    \"Can gout be cured permanently?\"\n",
    "]\n",
    "\n",
    "# TODO: Compare the different prompting strategies on these questions\n",
    "# For each question:\n",
    "# - Create prompts using each strategy\n",
    "# - Get responses from the LLM\n",
    "# - Store the results\n",
    "\n",
    "answers = [[\"Sugary drinks\", \"high purine foods\"],\n",
    "            [\"Colchicine\", \"Nonsteroidal anti-inflammatory drugs (NSAIDs)\"],\n",
    "            [\"Reduce uric acid levels\", \"Maintain healthy weight\"],\n",
    "            [\"Yes, uric acid build-up can cause gout\", \"Yes, diet choices can affect gout\"],\n",
    "            [\"No, it cannot be cured permanently.\", \"There is no cure, but management is possible.\"]]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for i in range(len(questions)):\n",
    "    question = questions[i]\n",
    "    a1, a2 = answers[i]\n",
    "\n",
    "    zero_shot_prompt = f\"Question: {question}\\nAnswer:\"\n",
    "\n",
    "    one_shot_prompt = f\"\"\"Question: {question}\n",
    "Answer: {a1}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    few_shot_prompt = f\"\"\"Question: {question}\n",
    "Answer: {a1}\n",
    "\n",
    "Question: {question}\n",
    "Answer: {a2}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    results[question] = {\"zero-shot\": zero_shot_prompt,\n",
    "    \"one-shot\": one_shot_prompt,\n",
    "    \"few-shot\": few_shot_prompt}\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c0525",
   "metadata": {},
   "source": [
    "## 5. Evaluating Responses\n",
    "\n",
    "Create a simple evaluation function to score the responses based on the presence of expected keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1ad9795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'zero_shot': 0.0, 'one_shot': 0.06666666666666667, 'few_shot': 0.20666666666666664}\n",
      "Scores per strategy:\n",
      "What foods should be avoided by patients with gout?: [0.0, 0.0, 0.16666666666666666]\n",
      "What medications are commonly prescribed for gout?: [0.0, 0.16666666666666666, 0.3333333333333333]\n",
      "How can gout flares be prevented?: [0.0, 0.0, 0.16666666666666666]\n",
      "Is gout related to diet?: [0.0, 0.16666666666666666, 0.16666666666666666]\n",
      "Can gout be cured permanently?: [0.0, 0.0, 0.2]\n"
     ]
    }
   ],
   "source": [
    "def score_response(response, keywords):\n",
    "    \"\"\"Score a response based on the presence of expected keywords\"\"\"\n",
    "    # TODO: Implement the score_response function\n",
    "    # Example implementation:\n",
    "    response = response.lower()\n",
    "    found_keywords = 0\n",
    "    for keyword in keywords:\n",
    "        if keyword.lower() in response:\n",
    "            found_keywords += 1\n",
    "    return found_keywords / len(keywords) if keywords else 0\n",
    "\n",
    "# Expected keywords for each question\n",
    "expected_keywords = {\n",
    "    \"What foods should be avoided by patients with gout?\": \n",
    "        [\"purine\", \"red meat\", \"seafood\", \"alcohol\", \"beer\", \"organ meats\"],\n",
    "    \"What medications are commonly prescribed for gout?\": \n",
    "        [\"nsaids\", \"colchicine\", \"allopurinol\", \"febuxostat\", \"probenecid\", \"corticosteroids\"],\n",
    "    \"How can gout flares be prevented?\": \n",
    "        [\"medication\", \"diet\", \"weight\", \"alcohol\", \"water\", \"exercise\"],\n",
    "    \"Is gout related to diet?\": \n",
    "        [\"yes\", \"purine\", \"food\", \"alcohol\", \"seafood\", \"meat\"],\n",
    "    \"Can gout be cured permanently?\": \n",
    "        [\"manage\", \"treatment\", \"lifestyle\", \"medication\", \"chronic\"]\n",
    "}\n",
    "\n",
    "# TODO: Score the responses and calculate average scores for each strategy\n",
    "# Determine which strategy performs best overall\n",
    "\n",
    "scores = {}\n",
    "# Score the responses\n",
    "for q, a in results.items():\n",
    "    zero = score_response(a[\"zero-shot\"], expected_keywords[q])\n",
    "    one = score_response(a[\"one-shot\"], expected_keywords[q])\n",
    "    few = score_response(a[\"few-shot\"], expected_keywords[q])\n",
    "    scores[q] = [zero, one, few]\n",
    "\n",
    "# Accumulate scores per strategy\n",
    "strategy_totals = {\"zero_shot\": 0, \"one_shot\": 0, \"few_shot\": 0}\n",
    "num_questions = len(scores)\n",
    "\n",
    "for s in scores.values():\n",
    "    strategy_totals[\"zero_shot\"] += s[0]\n",
    "    strategy_totals[\"one_shot\"] += s[1]\n",
    "    strategy_totals[\"few_shot\"] += s[2]\n",
    "\n",
    "# Compute averages\n",
    "average_scores = {\n",
    "    strategy: total / num_questions for strategy, total in strategy_totals.items()\n",
    "}\n",
    "\n",
    "print(average_scores)\n",
    "\n",
    "print(\"Scores per strategy:\")\n",
    "for strategy, score_list in scores.items():\n",
    "    print(f\"{strategy}: {score_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c92ea",
   "metadata": {},
   "source": [
    "## 6. Saving Results\n",
    "\n",
    "Save your results in a structured format for auto-grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c54007fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output to results/part_3/prompting_results.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# TODO: Save your results to results/part_3/prompting_results.txt\n",
    "# The file should include:\n",
    "# - Raw responses for each question and strategy\n",
    "# - Scores for each question and strategy\n",
    "# - Average scores for each strategy\n",
    "# - The best performing strategy\n",
    "\n",
    "# Example format:\n",
    "\"\"\"\n",
    "# Prompt Engineering Results\n",
    "\n",
    "## Question: What foods should be avoided by patients with gout?\n",
    "\n",
    "### Zero-shot response:\n",
    "[response text]\n",
    "\n",
    "### One-shot response:\n",
    "[response text]\n",
    "\n",
    "### Few-shot response:\n",
    "[response text]\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "## Scores\n",
    "\n",
    "```\n",
    "question,zero_shot,one_shot,few_shot\n",
    "what_foods_should,0.67,0.83,0.83\n",
    "what_medications_are,0.50,0.67,0.83\n",
    "how_can_gout,0.33,0.50,0.67\n",
    "is_gout_related,0.80,0.80,1.00\n",
    "can_gout_be,0.40,0.60,0.80\n",
    "\n",
    "average,0.54,0.68,0.83\n",
    "best_method,few_shot\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "output_path = \"results/part_3/prompting_results.txt\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(\"Prompt Engineering Results\\n\\n\")\n",
    "\n",
    "    for q, a in results.items():\n",
    "        f.write(f\"Question: {q}\\n\\n\")\n",
    "\n",
    "        f.write(\"Zero-shot response:\\n\")\n",
    "        f.write(a[\"zero-shot\"] + \"\\n\\n\")\n",
    "\n",
    "        f.write(\"One-shot response:\\n\")\n",
    "        f.write(a[\"one-shot\"] + \"\\n\\n\")\n",
    "\n",
    "        f.write(\"Few-shot response:\\n\")\n",
    "        f.write(a[\"few-shot\"] + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "    f.write(\"Scores\\n\\n\")\n",
    "    f.write(\"question,zero_shot,one_shot,few_shot\\n\")\n",
    "\n",
    "    for q, s_list in scores.items():\n",
    "        f.write(f\"{q},{s_list[0]},{s_list[1]},{s_list[2]}\\n\")\n",
    "\n",
    "    f.write(\"average,\" + str(average_scores[\"zero_shot\"]) + str(average_scores[\"one_shot\"]) + str(average_scores[\"few_shot\"]) + \"\\n\")\n",
    "    f.write(\"best_method,few_shot\")\n",
    "\n",
    "print(f\"Saved output to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8532e0c4",
   "metadata": {},
   "source": [
    "From the results, we can see that the few-shot prompting method has the best results with an average of 0.2067. This makes sense because by providing the model with as much information as possible, you are able to have a better output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4078975",
   "metadata": {},
   "source": [
    "## Progress Checkpoints\n",
    "\n",
    "1. **Prompting Templates**:\n",
    "   - [ ] Create zero-shot template\n",
    "   - [ ] Create one-shot template\n",
    "   - [ ] Create few-shot template\n",
    "   - [ ] Format templates with questions and examples\n",
    "\n",
    "2. **LLM API Integration**:\n",
    "   - [ ] Connect to the Hugging Face API\n",
    "   - [ ] Test with different prompts\n",
    "   - [ ] Handle API errors\n",
    "\n",
    "3. **Comparison and Evaluation**:\n",
    "   - [ ] Compare strategies on multiple questions\n",
    "   - [ ] Score responses based on keywords\n",
    "   - [ ] Determine the best strategy\n",
    "\n",
    "4. **Results and Documentation**:\n",
    "   - [ ] Save results in the required format\n",
    "   - [ ] Document your findings\n",
    "\n",
    "## What to Submit\n",
    "\n",
    "1. Your implementation in a Python script `utils/prompt_comparison.py` that:\n",
    "   - Defines the prompting templates\n",
    "   - Connects to the Hugging Face API\n",
    "   - Compares different prompting strategies\n",
    "   - Scores and evaluates the responses\n",
    "\n",
    "2. The results of your experiments in `results/part_3/prompting_results.txt` with the format shown above\n",
    "\n",
    "The auto-grader will check:\n",
    "1. That your results file contains the required sections\n",
    "2. That your scoring logic correctly identifies keyword presence\n",
    "3. That you've correctly calculated average scores\n",
    "4. That you've identified the best performing method"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
